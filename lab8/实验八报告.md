# 实验八报告

## 练习0：填写已有实验

本实验依赖实验2/3/4/5/6/7。已将之前实验的代码填入本实验中代码中有“LAB2”/“LAB3”/“LAB4”/“LAB5”/“LAB6” /“LAB7”的注释相应部分，并确保编译通过。

## 练习1: 完成读文件操作的实现

### 设计思路

`sfs_io_nolock` 函数主要负责在不加锁的情况下进行文件的读写操作。它是文件系统I/O的核心底层实现。
对于读操作，我们需要根据给定的 `offset`（文件内的偏移量）和 `alenp`（需要读写的长度），将文件内容从磁盘块读入到内存缓冲区 `buf` 中。

由于文件存储在磁盘上是以块（Block）为单位的（通常为4KB），而读写请求的 `offset` 和长度可能是任意的，因此我们需要处理以下三种情况：

1.  **起始部分不对齐**：如果 `offset` 不是块对齐的，我们需要读取包含 `offset` 的那个块的部分内容，直到该块的末尾。
2.  **中间对齐部分**：对于中间完整的块，我们可以直接整块读取，效率更高。
3.  **结束部分不对齐**：如果结束位置 `endpos` 不是块对齐的，我们需要读取最后一个块的开头部分，直到 `endpos`。

### 代码实现 (`kern/fs/sfs/sfs_inode.c`)

```c
static int
sfs_io_nolock(struct sfs_fs *sfs, struct sfs_inode *sin, void *buf, off_t offset, size_t *alenp, bool write) {
    // ... (前期检查代码略) ...

    // 计算起始块号和跨越的块数
    uint32_t blkno = offset / SFS_BLKSIZE;          // The NO. of Rd/Wr begin block
    uint32_t nblks = endpos / SFS_BLKSIZE - blkno;  // The size of Rd/Wr blocks

    // (1) 处理起始不对齐部分
    if ((offset % SFS_BLKSIZE) != 0) {
        // 计算第一块需要读写的大小
        // 如果跨越了块(nblks!=0)，则读到块末尾；否则读到endpos
        size = (nblks != 0) ? (SFS_BLKSIZE - (offset % SFS_BLKSIZE)) : (endpos - offset);
        
        // 加载对应的磁盘块号
        if ((ret = sfs_bmap_load_nolock(sfs, sin, blkno, &ino)) != 0) {
            goto out;
        }
        // 执行读写操作 (sfs_buf_op 封装了 sfs_rbuf/sfs_wbuf)
        if ((ret = sfs_buf_op(sfs, buf, size, ino, offset % SFS_BLKSIZE)) != 0) {
            goto out;
        }
        alen += size;
        // 如果只在同一个块内，处理完就结束
        if (nblks == 0) {
            goto out;
        }
        // 更新缓冲区指针、块号、剩余块数
        buf += size, blkno ++, nblks --;
    }

    // (2) 处理中间对齐的完整块
    size = SFS_BLKSIZE;
    while (nblks != 0) {
        if ((ret = sfs_bmap_load_nolock(sfs, sin, blkno, &ino)) != 0) {
            goto out;
        }
        // 直接读写整块 (sfs_block_op 封装了 sfs_rblock/sfs_wblock)
        if ((ret = sfs_block_op(sfs, buf, ino, 1)) != 0) {
            goto out;
        }
        alen += size, buf += size, blkno ++, nblks --;
    }

    // (3) 处理结束不对齐部分
    if ((size = endpos % SFS_BLKSIZE) != 0) {
        if ((ret = sfs_bmap_load_nolock(sfs, sin, blkno, &ino)) != 0) {
            goto out;
        }
        // 从块头开始读写 size 字节
        if ((ret = sfs_buf_op(sfs, buf, size, ino, 0)) != 0) {
            goto out;
        }
        alen += size;
    }

out:
    *alenp = alen;
    // ...
}
```

## 练习2: 完成基于文件系统的执行程序机制的实现

### 设计思路

`load_icode` 函数的功能是加载并解析一个 ELF 格式的二进制程序文件，将其映射到进程的内存空间中，并准备好执行环境（如堆栈、参数等）。
在 Lab 8 中，我们需要利用文件系统接口来读取程序文件，而不是像之前那样直接从内存读取。

主要步骤如下：
1.  **创建内存管理结构**：调用 `mm_create` 和 `setup_pgdir` 初始化进程的内存空间。
2.  **读取 ELF Header**：使用 `load_icode_read`（封装了 `sysfile_read`）从文件描述符 `fd` 读取 ELF 头。
3.  **校验 ELF**：检查魔数等是否合法。
4.  **加载 Program Headers**：遍历 ELF 的 Program Headers，找到类型为 `ELF_PT_LOAD` 的段。
5.  **内存映射与内容复制**：
    *   根据段的权限设置内存页的权限（R/W/X）。
    *   调用 `mm_map` 建立虚拟内存映射。
    *   为段分配物理页 (`pgdir_alloc_page`)。
    *   使用 `load_icode_read` 将文件中对应段的数据读取到分配的物理页中。
    *   处理 BSS 段（文件大小 < 内存大小的部分），将其清零。
6.  **设置用户栈**：建立用户栈的内存映射，并将命令行参数 (`argc`, `argv`) 压入栈中。
7.  **切换页表**：更新进程的 `cr3` 寄存器。
8.  **设置 Trapframe**：设置用户态的寄存器状态（`epc` 指向入口点，`sp` 指向用户栈顶）。

### 代码实现 (`kern/process/proc.c`)

```c
static int
load_icode(int fd, int argc, char **kargv)
{
    // ... (初始化 mm, pgdir, 读取 ELF Header 代码略) ...

    struct proghdr __ph, *ph = &__ph;
    uint32_t vm_flags, perm, phnum;
    for (phnum = 0; phnum < elf->e_phnum; phnum ++) {
        // 读取 Program Header
        off_t phoff = elf->e_phoff + sizeof(struct proghdr) * phnum;
        if ((ret = load_icode_read(fd, ph, sizeof(struct proghdr), phoff)) != 0) {
            goto bad_cleanup_mmap;
        }
        if (ph->p_type != ELF_PT_LOAD) {
            continue ;
        }
        // ... (权限设置代码略) ...

        // 建立内存映射
        if ((ret = mm_map(mm, ph->p_va, ph->p_memsz, vm_flags, NULL)) != 0) {
            goto bad_cleanup_mmap;
        }
        
        // 复制段内容到内存
        off_t offset = ph->p_offset;
        size_t off, size;
        uintptr_t start = ph->p_va, end, la = ROUNDDOWN(start, PGSIZE);

        ret = -E_NO_MEM;
        end = ph->p_va + ph->p_filesz;
        
        // 逐页分配并读取数据
        while (start < end) {
            if ((page = pgdir_alloc_page(mm->pgdir, la, perm)) == NULL) {
                ret = -E_NO_MEM;
                goto bad_cleanup_mmap;
            }
            off = start - la, size = PGSIZE - off, la += PGSIZE;
            if (end < la) {
                size -= la - end;
            }
            // 从文件读取数据到物理页对应的内核虚拟地址
            if ((ret = load_icode_read(fd, page2kva(page) + off, size, offset)) != 0) {
                goto bad_cleanup_mmap;
            }
            start += size, offset += size;
        }

        // 处理 BSS 段 (MemSize > FileSize 的部分清零)
        end = ph->p_va + ph->p_memsz;
        if (start < la) {
            // 如果当前页还有剩余空间，先清零
            if (start == end) {
                continue ;
            }
            off = start + PGSIZE - la, size = PGSIZE - off;
            if (end < la) {
                size -= la - end;
            }
            memset(page2kva(page) + off, 0, size);
            start += size;
            assert((end < la && start == end) || (end >= la && start == la));
        }
        
        // 为剩余的 BSS 分配新页并清零
        while (start < end) {
            if ((page = pgdir_alloc_page(mm->pgdir, la, perm)) == NULL) {
                ret = -E_NO_MEM;
                goto bad_cleanup_mmap;
            }
            off = start - la, size = PGSIZE - off, la += PGSIZE;
            if (end < la) {
                size -= la - end;
            }
            memset(page2kva(page) + off, 0, size);
            start += size;
        }
    }
    // ... (设置用户栈和 Trapframe 代码略) ...
}
```

## 知识点总结

### 实验中的重要知识点

1.  **虚拟文件系统 (VFS)**
    *   **含义**：VFS 是操作系统内核中的一个软件层，它为用户空间程序提供了一个统一的文件系统接口，屏蔽了底层具体文件系统（如 SFS, FAT, ext4）的差异。
    *   **与OS原理的关系**：对应操作系统原理中“文件系统接口”与“文件系统实现”的分离。VFS 定义了通用的数据结构（如 `inode`, `file`, `dentry`, `superblock`）和操作接口（如 `vop_read`, `vop_write`），使得内核可以透明地支持多种文件系统。

2.  **索引节点 (Inode)**
    *   **含义**：Inode 是文件系统中的核心数据结构，用于存储文件的元数据（如文件大小、类型、权限、数据块位置等），但不包含文件名。
    *   **与OS原理的关系**：对应文件系统实现中的“文件控制块 (FCB)”。在 ucore 中，`sfs_disk_inode` 是磁盘上的 inode，`sfs_inode` 是内存中的 inode，两者通过 `sfs_load_inode` 等函数进行转换。

3.  **文件描述符 (File Descriptor)**
    *   **含义**：进程级别的文件句柄，是一个整数，用于索引进程打开文件表。
    *   **与OS原理的关系**：对应进程管理与文件系统的交互接口。进程通过文件描述符操作文件，内核通过文件描述符找到对应的 `file` 结构体，进而找到 `inode`。

4.  **ELF 文件加载 (`load_icode`)**
    *   **含义**：将磁盘上的可执行文件加载到内存中并执行的过程。
    *   **与OS原理的关系**：对应“程序执行”和“进程创建”。涉及读取文件头、解析段表、建立虚拟内存映射 (`mm_map`)、分配物理页 (`pgdir_alloc_page`) 以及设置用户栈和上下文。

### 实验中未涉及的OS原理重要知识点

1.  **日志与崩溃恢复 (Journaling & Crash Recovery)**
    *   SFS 是一个简单的文件系统，没有实现日志机制（Journaling）。在系统崩溃（如断电）时，文件系统可能会处于不一致状态，且难以恢复。现代文件系统（如 ext4, NTFS）通常都有日志功能。

2.  **文件系统缓存 (Buffer Cache / Page Cache)**
    *   虽然 ucore 有简单的缓冲区，但没有实现完善的页缓存（Page Cache）或块缓存（Buffer Cache）机制来优化磁盘 I/O 性能。现代 OS 会利用空闲内存缓存文件数据，减少磁盘访问。

3.  **异步 I/O (Asynchronous I/O)**
    *   本实验中的文件读写是同步阻塞的。现代 OS 支持异步 I/O，允许进程在 I/O 完成前继续执行其他任务。

4.  **访问控制列表 (ACL)**
    *   SFS 只有非常基础的权限控制，不支持复杂的 ACL。

## 扩展练习 Challenge1：完成基于“UNIX的PIPE机制”的设计方案

### 概要设计

UNIX 管道（Pipe）是一种进程间通信机制，允许一个进程的输出作为另一个进程的输入。在 ucore 中实现管道，可以将其视为一种特殊的文件类型。

### 数据结构设计

我们需要定义一个管道的内存结构，用于管理缓冲区和同步状态。

```c
#define PIPE_SIZE 4096

struct pipe_inode_info {
    mutex_t mutex;              // 互斥锁，保护管道数据
    condvar_t cv_read;          // 读条件变量，缓冲区空时等待
    condvar_t cv_write;         // 写条件变量，缓冲区满时等待
    char buffer[PIPE_SIZE];     // 循环缓冲区
    unsigned int n_read;        // 读指针位置
    unsigned int n_write;       // 写指针位置
    unsigned int cnt;           // 当前缓冲区中的字节数
    int readers;                // 读者计数
    int writers;                // 写者计数
};
```

### 接口语义

1.  **`pipe(int fd[2])` 系统调用**：
    *   创建两个文件描述符，`fd[0]` 用于读，`fd[1]` 用于写。
    *   在内核中分配一个 `pipe_inode_info` 结构。
    *   将两个文件描述符都指向这个管道 inode。
    *   `fd[0]` 打开方式为只读，`fd[1]` 打开方式为只写。

2.  **`read` 操作**：
    *   获取互斥锁。
    *   如果缓冲区为空 (`cnt == 0`)：
        *   如果写者计数为 0 (`writers == 0`)，说明管道已关闭，返回 0 (EOF)。
        *   否则，在 `cv_read` 上等待。
    *   从缓冲区读取数据，更新 `n_read` 和 `cnt`。
    *   唤醒 `cv_write` 上的等待者。
    *   释放锁，返回读取字节数。

3.  **`write` 操作**：
    *   获取互斥锁。
    *   如果缓冲区已满 (`cnt == PIPE_SIZE`)：
        *   如果读者计数为 0 (`readers == 0`)，发送 SIGPIPE 信号（或返回错误）。
        *   否则，在 `cv_write` 上等待。
    *   向缓冲区写入数据，更新 `n_write` 和 `cnt`。
    *   唤醒 `cv_read` 上的等待者。
    *   释放锁，返回写入字节数。

4.  **`close` 操作**：
    *   减少 `readers` 或 `writers` 计数。
    *   如果计数归零，唤醒相应的等待者（让它们退出等待并感知 EOF 或 Broken Pipe）。
    *   如果所有引用都消失，释放 `pipe_inode_info`。

## 扩展练习 Challenge2：完成基于“UNIX的软连接和硬连接机制”的设计方案

### 概要设计

*   **硬链接 (Hard Link)**：多个目录项指向同一个 inode。删除一个链接只是减少引用计数，只有计数为 0 时才真正删除文件。
*   **软链接 (Symbolic Link)**：一个特殊的文件，其内容是指向另一个文件的路径。

### 数据结构设计

**硬链接**：
需要在磁盘 inode (`sfs_disk_inode`) 和内存 inode (`sfs_inode`) 中增加引用计数 `nlinks`。

```c
struct sfs_disk_inode {
    // ... 原有字段 ...
    uint16_t nlinks;            // 硬链接计数
};
```

**软链接**：
需要定义一种新的文件类型 `SFS_TYPE_LINK`。

```c
#define SFS_TYPE_LINK 2
```

### 接口语义

1.  **`link(oldpath, newpath)` (硬链接)**：
    *   解析 `oldpath` 找到对应的 inode。
    *   检查 inode 类型（通常不允许对目录创建硬链接以避免环）。
    *   在 `newpath` 的父目录下创建一个新目录项 (entry)。
    *   将新目录项的 inode 编号设置为 `oldpath` 的 inode 编号。
    *   增加该 inode 的 `nlinks` 计数，并写回磁盘。

2.  **`symlink(target, linkpath)` (软链接)**：
    *   创建一个新文件，路径为 `linkpath`。
    *   设置新文件的类型为 `SFS_TYPE_LINK`。
    *   将 `target` 字符串作为文件内容写入新文件。

3.  **`unlink(path)`**：
    *   解析 `path` 找到目录项和 inode。
    *   从父目录中删除该目录项。
    *   减少 inode 的 `nlinks` 计数。
    *   如果 `nlinks == 0` 且没有进程打开该文件，则释放 inode 和数据块。

4.  **`open` (软链接处理)**：
    *   在路径解析过程中，如果遇到 `SFS_TYPE_LINK` 类型的 inode。
    *   读取其内容（目标路径）。
    *   递归调用路径解析函数解析目标路径。
    *   需要设置最大递归深度（如 5 或 8）以防止死循环。

### 同步互斥处理

*   **硬链接**：修改 `nlinks` 时需要加锁（如 `sfs_io_nolock` 中使用的锁或 inode 锁），防止并发修改导致计数错误。
*   **软链接**：读取软链接内容并跳转时，需要注意路径解析过程中的锁顺序，防止死锁。通常在解析完一个分量后释放锁，再获取下一个分量的锁。
